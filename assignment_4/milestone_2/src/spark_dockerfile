FROM ubuntu:latest

RUN apt-get -y update && apt-get install -y default-jdk python3
RUN apt-get install -y python3-dev python3-pip
RUN apt-get install -y vim
RUN apt-get install -y time
RUN python3 -m pip install --upgrade pip
RUN python3 -m pip install --upgrade pyspark
RUN python3 -m pip install --upgrade requests
RUN python3 -m pip install --upgrade pandas

# Note, I have added several other packages that provide networking utilities like
# ping, nslookup, ifconfig etc.
RUN apt-get -y update && apt-get install -y net-tools wget dnsutils iputils-ping iputils-tracepath iputils-arping iputils-clockdiff

# Here we are hardcoding the download mirror and the spark version. I am sure
# there will be another and better way to do this
RUN wget https://apache.osuosl.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2-scala2.13.tgz
RUN zcat spark-3.2.0-bin-hadoop3.2-scala2.13.tgz | tar xpof -

COPY spark-env.sh /spark-3.2.0-bin-hadoop3.2-scala2.13/conf/
COPY spark-worker.conf /spark-3.2.0-bin-hadoop3.2-scala2.13/conf/
COPY spark-driver.conf /spark-3.2.0-bin-hadoop3.2-scala2.13/conf/
COPY mapreduce.py /spark-3.2.0-bin-hadoop3.2-scala2.13
COPY run_iters.sh /
RUN chmod +x run_iters.sh

# Now we set environment variable that we will need in the container at runtime
ENV SPARK_HOME=~/Apps/spark-3.2.0-bin-hadoop3.2-scala2.13
ENV PATH=${PATH}:${SPARK_HOME}/sbin:${SPARK_HOME}/bin